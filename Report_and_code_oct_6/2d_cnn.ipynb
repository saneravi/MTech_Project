{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from skimage import io, transform\n",
    "from tensorflow.keras import layers, models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read created ecg images\n",
    "Default_path = '/.'\n",
    "\n",
    "# Call the function that reads the picture to get the dataset of pictures and labels\n",
    "def read_img(path=Default_path, w=1200, h=400, c=3):\n",
    "    cate = [path + x for x in os.listdir(path) if os.path.isdir(path + x)]\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for idx, folder in enumerate(cate):\n",
    "        for im in glob.glob(folder + '/*.jpg'):\n",
    "            # print('reading the images:%s' % (im))\n",
    "            img = cv2.imread(im)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "            imgs.append(img)\n",
    "            labels.append(idx)\n",
    "    \n",
    "    print(\"ECG Images Read!!!\")\n",
    "    return np.array(imgs), np.asarray(labels, np.int32)\n",
    "\n",
    "\n",
    "# Resize all pics\n",
    "w = 1800\n",
    "h = 600\n",
    "c = 3\n",
    "path = './ptbdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG Images Read!!!\n"
     ]
    }
   ],
   "source": [
    "data, label = read_img(path=path, w=w, h=h, c=c) # call the function to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing dataset\n",
    "\n",
    "num_example = data.shape[0]  # the total number of pictures\n",
    "arr = np.arange(num_example)  # np.arange (start value, end value, step size)\n",
    "np.random.shuffle(arr)  # Rearrange the assignment after scrambling\n",
    "data = data[arr]\n",
    "label = label[arr]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(data, label, test_size = 0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to shuffle the order in which data to be fed for training\n",
    "num_example = train_images.shape[0]  # the total number of pictures\n",
    "arr1 = np.arange(num_example)  # np.arange (start value, end value, step size)\n",
    "np.random.shuffle(arr1)  # Rearrange the assignment after scrambling\n",
    "train_images = train_images[arr1]\n",
    "train_labels = train_labels[arr1]\n",
    "\n",
    "# to shuffle the order in which data to be fed for testing\n",
    "num_example = test_images.shape[0]  # the total number of pictures\n",
    "arr2 = np.arange(num_example)  # np.arange (start value, end value, step size)\n",
    "np.random.shuffle(arr2)  # Rearrange the assignment after scrambling\n",
    "test_images = test_images[arr2]\n",
    "test_labels = test_labels[arr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create the convolutional base \n",
    "    \n",
    "cnn_model = models.Sequential()\n",
    "cnn_model.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (w, h, c)))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "\n",
    "# cnn_model.summary()\n",
    "\n",
    "#%% Add Dense layers on top\n",
    "\n",
    "cnn_model.add(layers.Flatten())\n",
    "cnn_model.add(layers.Dense(64, activation = 'relu'))\n",
    "cnn_model.add(layers.Dense(number_of_classes))\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Compile and train (fit) the model\n",
    "\n",
    "cnn_model.compile(optimizer = 'adam',\n",
    "              loss = losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = cnn_model.fit(train_images, train_labels, epochs = number_of_iterations, \n",
    "                    validation_data = (test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Evaluate the model\n",
    "\n",
    "plt.plot(history.history['accuracy'], label = 'Training_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.ylim([0.5, 1])\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(test_images,  test_labels, verbose = 2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "cnn_model.save(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
